{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb459685",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"arxiv.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593668e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 39541577\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50110cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0002Title: Working and Assembly Modes of the Agile Eye\n",
      "Abstract: This paper deals with the in-depth kinematic analysis of a special spherical parallel wrist, called the Agile Eye. The Agile Eye is a three-legged spherical parallel robot with revolute joints in which all pairs of adjacent joint axes are orthogonal. Its most peculiar feature, demonstrated in this paper for the first time, is that its (orientation) workspace is unlimited and flawed only by six singularity curves (rather than surfaces). Furthermore, these curves correspond to self-motions of the mobile platform. This paper also demonstrates that, unlike for any other such complex spatial robots, the four solutions to the direct kinematics of the Agile Eye (assembly modes) have a simple geometric relationship with the eight solutions to the inverse kinematics (working modes).\u0003\n",
      "\n",
      "\u0002Title: Kinematic and stiffness analysis of the Orthoglide, a PKM with simple, regular workspace and homogeneous performances\n",
      "Abstract: The Orthoglide is a Delta-type PKM dedicated to 3-axis rapid machining applications that was originally developed at IRCCyN in 2000-2001 to meet the advantages of both serial 3-axis machines (regular workspace and homogeneous performances) and parallel kinematic architectures (good dynamic performances and stiffness). This machine has three fixed parallel linear joints that are mounted orthogonally. The geometric parameters of the Orthoglide were defined as function of the size of a prescribed cubic Cartesian workspace that is free of singularities and internal collision. The interesting features of the Orthoglide are a regular Cartesian workspace shape, uniform performances in all directions and good compactness. In this paper, a new method is proposed to analyze the stiffness of overconstrained Delta-type manipulators, such as the Orthoglide. The Orthoglide is then benchmarked according to geometric, kinematic and stiffness criteria: workspace to footprint ratio, velocity and force transmission factors, sensitivity to geometric errors, torsional stiffness and translational stiffness.\u0003\n",
      "\n",
      "\u0002Title: Architecture Optimization of a 3-DOF Translational Parallel Mechanism for Machining Applications, the Orthoglide\n",
      "Abstract: This paper addresses the architecture optimization of a 3-DOF translational parallel mechanism designed for machining applications. The design optimization is conducted on the basis of a prescribed Cartesian workspace with prescribed kinetostatic performances. The resulting machine, the Orthoglide, features three fixed parallel linear joints which are mounted orthogonally and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The interesting features of the Orthoglide are a regular Cartesian workspace shape, uniform performances in all directions and good compactness. A small-scale prototype of the Orthoglide under development is presented at the end of this paper.\u0003\n",
      "\n",
      "\u0002Title: Parametric stiffness analysis of the Orthoglide\n",
      "Abstract: This paper presents a parametric stiffness analysis of the Orthoglide. A compliant modeling and a symbolic expression of the stiffness matrix are conducted. This allows a simple systematic analysis of the influence of the geometric design parameters and to quickly identify the critical link parameters. Our symbolic model is used to display the stiffest areas of the workspace for a specific machining task. Our approach can be applied to any parallel manipulator for which stiffness is a critical issue.\u0003\n",
      "\n",
      "\u0002Title: Kinematic analysis of the 3-RPR parallel manipulator\n",
      "Abstract: The aim of this paper is the kinematic study of a 3-RPR planar parallel manipulator where the three fixed revolute joints are actuated. The direct and inverse kinematic problem as well as the singular configuration is characterized. On parallel singular configurations, the motion produce by the mobile platform can be compared to the Reuleaux straight-line mechanism.\u0003\n",
      "\n",
      "\u0002Title: A Six Degree-Of-Freedom Haptic Device Based On The Orthoglide And A Hybrid Agile Eye\n",
      "Abstract: This paper is devoted to the kinematic design of a new six degree-of-freedom haptic device using two parallel mechanisms. The first one, called orthoglide, provides the translation motions and the second one, called agile eye, produces the rotational motions. These two motions are decoupled to simplify the direct and inverse kinematics, as it is needed for real-time control. To reduce the inertial load, the motors are fixed on the base and a transmission with two universal joints is used to transmit the rotational motions from the base to the end-effector. Two alternative wrists are proposed (i), the agile eye with three degrees of freedom or (ii) a hybrid wrist made by the assembly of a two-dof agile eye with a rotary motor. The last one is optimized to increase its stiffness and to decrease the number of moving parts.\u0003\n",
      "\n",
      "\u0002Title: A Comparative Study of Parallel Kinematic Architectures for Machining Applications\n",
      "Abstract: Parallel kinemati\n"
     ]
    }
   ],
   "source": [
    "print(text[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a64d88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0002\u0003\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~£§°ÆÈÖØÜàáãäåæçèéêíïñóôöøüČıōřśŠťΠΦαδεθκμπρψ\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a7038bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "BOS = \"\\x02\"\n",
    "EOS = \"\\x03\"\n",
    "print(BOS in chars and EOS in chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe84ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75, 72, 76, 86, 80, 72, 81, 71, 82, 93, 68]\n",
      "heismendoza\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"heismendoza\"))\n",
    "print(decode(encode(\"heismendoza\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5006d3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39541577]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcff4487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 55, 76, 87, 79, 72, 29,  3, 58, 82, 85, 78, 76, 81, 74,  3, 68, 81,\n",
      "        71,  3, 36, 86, 86, 72, 80, 69, 79, 92,  3, 48, 82, 71, 72, 86,  3, 82,\n",
      "        73,  3, 87, 75, 72,  3, 36, 74, 76, 79, 72,  3, 40, 92, 72,  2, 36, 69,\n",
      "        86, 87, 85, 68, 70, 87, 29,  3, 55, 75, 76, 86,  3, 83, 68, 83, 72, 85,\n",
      "         3, 71, 72, 68, 79, 86,  3, 90, 76, 87, 75,  3, 87, 75, 72,  3, 76, 81,\n",
      "        16, 71, 72, 83, 87, 75,  3, 78, 76, 81])\n"
     ]
    }
   ],
   "source": [
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b216d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d797128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35587419]), torch.Size([3954158]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23e17555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 55, 76, 87, 79, 72, 29,  3, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6b489fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([0]) the target: 55\n",
      "when input is tensor([ 0, 55]) the target: 76\n",
      "when input is tensor([ 0, 55, 76]) the target: 87\n",
      "when input is tensor([ 0, 55, 76, 87]) the target: 79\n",
      "when input is tensor([ 0, 55, 76, 87, 79]) the target: 72\n",
      "when input is tensor([ 0, 55, 76, 87, 79, 72]) the target: 29\n",
      "when input is tensor([ 0, 55, 76, 87, 79, 72, 29]) the target: 3\n",
      "when input is tensor([ 0, 55, 76, 87, 79, 72, 29,  3]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ae39cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "071bcd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[78,  3, 72, 73, 73, 76, 70, 76],\n",
      "        [81, 71,  3, 73, 76, 91, 72, 71],\n",
      "        [83, 85, 82, 68, 70, 75,  3, 76],\n",
      "        [ 3, 73, 82, 85,  3, 68,  3, 37]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 3, 72, 73, 73, 76, 70, 76, 72],\n",
      "        [71,  3, 73, 76, 91, 72, 71, 16],\n",
      "        [85, 82, 68, 70, 75,  3, 76, 81],\n",
      "        [73, 82, 85,  3, 68,  3, 37, 68]])\n",
      "----\n",
      "when input is [78] the target: 3\n",
      "when input is [78, 3] the target: 72\n",
      "when input is [78, 3, 72] the target: 73\n",
      "when input is [78, 3, 72, 73] the target: 73\n",
      "when input is [78, 3, 72, 73, 73] the target: 76\n",
      "when input is [78, 3, 72, 73, 73, 76] the target: 70\n",
      "when input is [78, 3, 72, 73, 73, 76, 70] the target: 76\n",
      "when input is [78, 3, 72, 73, 73, 76, 70, 76] the target: 72\n",
      "when input is [81] the target: 71\n",
      "when input is [81, 71] the target: 3\n",
      "when input is [81, 71, 3] the target: 73\n",
      "when input is [81, 71, 3, 73] the target: 76\n",
      "when input is [81, 71, 3, 73, 76] the target: 91\n",
      "when input is [81, 71, 3, 73, 76, 91] the target: 72\n",
      "when input is [81, 71, 3, 73, 76, 91, 72] the target: 71\n",
      "when input is [81, 71, 3, 73, 76, 91, 72, 71] the target: 16\n",
      "when input is [83] the target: 85\n",
      "when input is [83, 85] the target: 82\n",
      "when input is [83, 85, 82] the target: 68\n",
      "when input is [83, 85, 82, 68] the target: 70\n",
      "when input is [83, 85, 82, 68, 70] the target: 75\n",
      "when input is [83, 85, 82, 68, 70, 75] the target: 3\n",
      "when input is [83, 85, 82, 68, 70, 75, 3] the target: 76\n",
      "when input is [83, 85, 82, 68, 70, 75, 3, 76] the target: 81\n",
      "when input is [3] the target: 73\n",
      "when input is [3, 73] the target: 82\n",
      "when input is [3, 73, 82] the target: 85\n",
      "when input is [3, 73, 82, 85] the target: 3\n",
      "when input is [3, 73, 82, 85, 3] the target: 68\n",
      "when input is [3, 73, 82, 85, 3, 68] the target: 3\n",
      "when input is [3, 73, 82, 85, 3, 68, 3] the target: 37\n",
      "when input is [3, 73, 82, 85, 3, 68, 3, 37] the target: 68\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74673738",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "351cad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 142])\n",
      "tensor(5.9305, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06f987d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0002ŠōYθFOZvj/uøōíqLjiÖCï5k\"ρGT=.l9$ñjX}0g9ojsθaÈ`Ko~WLκå§bèČΠcæ+zWäεHçsBLZå§κρs£[i/αψťG~\\àıεTΦŠ+$O0Ah2s\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "544060fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "620617c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.346942901611328\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(1000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ab6469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0002ä&%κ\\u°fsñ3g9$y,RJéq*B@ã!wèö%Uw°c)pS~£ōíXκQC'Ru0q*äI=:śXρρŠSDPÖzr8~íñō]ANJÈE?^\u0002J\"-nnbkČápκ0Neá6!íΠwrfeμmE4:ôqJï^n;ô!BČıPz0fa^n\\ØRM#2tjôQôLA;£\n",
      "AřRmeameθiťWÜ\"/]3ı\"/'`yçæ,h@QÆäz+ř:δat@£, wpYIIθ3êHÜ]5Sq\n",
      "7θU_£\u0002äťκ)§\u00029e:öüψD{övjn-FTÖvDa1ü\"IFøeQ^8O-k?ΠV\u00032ı,rt.üPô2Ö6\"ç-eW;vθçíČóæ\n",
      "@ťRóU'WYjIVÖ3Tes4v°PıF\u0002GZóÈZ,°Ø&èãBLČρq[ê+zs4ociiø2°åiGT9D3mΦgC>23πñ4;F\\üUμá{z\"ČáśUrU{?V\\wWäGPV$lεñδ=\u0002àBIıïΦDø@day?íÈ§§i5HgcOuf(gQøG%θäÖcťśts\u0003ïfDvVhQ4ébj3ıÆäá6bñ=kV)3TLκε8ÆUnpæbw|ü\\X.,8dMP'2UIta;RθO\\ôřø%ç1Kñ#*iΦ°0åwh$ψ|Kō\u0003FCo9^\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e879101",
   "metadata": {},
   "source": [
    "self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d98ea731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(13337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f568ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6474,  0.2304],\n",
       "        [-0.3166,  0.5689],\n",
       "        [ 0.0301,  1.6746],\n",
       "        [-0.4068,  0.4601],\n",
       "        [-1.4540,  1.1923],\n",
       "        [ 0.2952,  0.4682],\n",
       "        [-0.2468,  0.8997],\n",
       "        [ 1.7527, -1.1156]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first batch x\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7b605f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1: for loops\n",
    "# x[b, t] = mean{i<=t} x[b, i]\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[b, :t+1] # t, C\n",
    "        xbow[b, t] = torch.mean(x_prev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d420fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6474,  0.2304],\n",
       "        [-0.4820,  0.3996],\n",
       "        [-0.3113,  0.8246],\n",
       "        [-0.3352,  0.7335],\n",
       "        [-0.5589,  0.8252],\n",
       "        [-0.4166,  0.7657],\n",
       "        [-0.3923,  0.7849],\n",
       "        [-0.1242,  0.5473]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: simple matrix\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "718b3885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6474,  0.2304],\n",
       "        [-0.4820,  0.3996],\n",
       "        [-0.3113,  0.8246],\n",
       "        [-0.3352,  0.7335],\n",
       "        [-0.5589,  0.8252],\n",
       "        [-0.4166,  0.7657],\n",
       "        [-0.3923,  0.7849],\n",
       "        [-0.1242,  0.5473]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "xbow3 = wei @ x\n",
    "xbow3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3618a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention\n",
    "torch.manual_seed(13337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C) # (B, T, C)\n",
    "\n",
    "# single head self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "v = value(x) # (B, T, head_size)\n",
    "out = wei @ v # (B, T, T) @ (B, T, head_size) --> (B, T, head_size)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "996cd703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled attention\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size **-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5533dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0419)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ff87b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0594)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f886c712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9832)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "deb00d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09ddf218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b9e4241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check if all methods equal\n",
    "print(torch.allclose(xbow, xbow2))\n",
    "print(torch.allclose(xbow2, xbow3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fb72f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / a.sum(1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7987e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
